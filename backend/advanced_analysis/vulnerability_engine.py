"""Advanced Vulnerability Analysis Engine - Orchestrates all analysis components"""
from typing import Dict, List, Any
import logging
from .ast_parser import MultiLanguageASTParser
from .unified_representation import UnifiedIRGenerator
from .taint_analysis import TaintAnalyzer
from .pattern_recognition import PatternRecognizer
from .cross_language_detector import CrossLanguageSecurityDetector

logger = logging.getLogger(__name__)

class AdvancedVulnerabilityEngine:
    """Main engine that orchestrates advanced vulnerability analysis"""
    
    def __init__(self):
        self.ast_parser = MultiLanguageASTParser()
        self.ir_generator = UnifiedIRGenerator()
        self.taint_analyzer = TaintAnalyzer()
        self.pattern_recognizer = PatternRecognizer()
        self.cross_lang_detector = CrossLanguageSecurityDetector()
    
    async def analyze_file(self, code: str, language: str, file_path: str = '') -> Dict[str, Any]:
        """Analyze a single file with advanced algorithms"""
        try:
            logger.info(f"Starting advanced analysis for {file_path} ({language})")
            
            result = {
                'file_path': file_path,
                'language': language,
                'vulnerabilities': [],
                'analysis_stages': {},
                'success': True
            }
            
            # Stage 1: AST Parsing
            logger.info(f"Stage 1: Parsing AST for {language}")
            ast_data = self.ast_parser.parse(code, language)
            result['analysis_stages']['ast_parsing'] = {
                'completed': True,
                'functions_found': len(ast_data.get('functions', [])),
                'variables_found': len(ast_data.get('variables', [])),
                'imports_found': len(ast_data.get('imports', []))
            }
            
            # Stage 2: IR Generation
            logger.info(f"Stage 2: Generating unified IR")
            ir_data = self.ir_generator.generate(ast_data)
            result['analysis_stages']['ir_generation'] = {
                'completed': True,
                'function_definitions': len(ir_data.get('function_definitions', [])),
                'external_calls': len(ir_data.get('external_calls', [])),
                'data_sources': len(ir_data.get('data_sources', [])),
                'data_sinks': len(ir_data.get('data_sinks', []))
            }
            
            # Stage 3: Pattern Recognition
            logger.info(f"Stage 3: Applying pattern recognition")
            pattern_vulns = self.pattern_recognizer.analyze(code, language, file_path)
            result['vulnerabilities'].extend(pattern_vulns)
            result['analysis_stages']['pattern_recognition'] = {
                'completed': True,
                'patterns_matched': len(pattern_vulns)
            }
            
            # Stage 4: Taint Analysis
            logger.info(f"Stage 4: Performing taint analysis")
            taint_result = self.taint_analyzer.analyze(ir_data)
            if 'vulnerabilities' in taint_result:
                result['vulnerabilities'].extend(taint_result['vulnerabilities'])
            result['analysis_stages']['taint_analysis'] = {
                'completed': True,
                'tainted_variables': len(taint_result.get('tainted_variables', [])),
                'taint_flows': len(taint_result.get('taint_flows', [])),
                'vulnerabilities_found': len(taint_result.get('vulnerabilities', [])),
                'risk_score': taint_result.get('risk_score', 0)
            }
            
            # Store IR for cross-language analysis
            result['ir_data'] = ir_data
            result['taint_data'] = taint_result
            
            logger.info(f"Completed analysis: {len(result['vulnerabilities'])} vulnerabilities found")
            return result
            
        except Exception as e:
            logger.error(f"Error in advanced analysis: {str(e)}")
            return {
                'file_path': file_path,
                'language': language,
                'vulnerabilities': [],
                'success': False,
                'error': str(e)
            }
    
    async def analyze_repository(self, files: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze entire repository with cross-language detection"""
        try:
            logger.info(f"Starting repository analysis for {len(files)} files")
            
            all_file_results = []
            all_vulnerabilities = []
            all_ir_data = []
            
            # Analyze each file
            for file_data in files:
                code = file_data.get('content', '')
                language = file_data.get('language', 'unknown')
                file_path = file_data.get('path', '')
                
                file_result = await self.analyze_file(code, language, file_path)
                all_file_results.append(file_result)
                
                if file_result.get('success'):
                    all_vulnerabilities.extend(file_result.get('vulnerabilities', []))
                    if 'ir_data' in file_result:
                        all_ir_data.append(file_result['ir_data'])
            
            # Stage 5: Cross-Language Analysis
            logger.info(f"Stage 5: Performing cross-language analysis")
            cross_lang_vulns = []
            if len(all_ir_data) > 1:
                cross_lang_vulns = self.cross_lang_detector.analyze(all_ir_data)
                all_vulnerabilities.extend(cross_lang_vulns)
            
            # Calculate statistics
            severity_counts = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0, 'info': 0}
            for vuln in all_vulnerabilities:
                severity = vuln.get('severity', 'info').lower()
                if severity in severity_counts:
                    severity_counts[severity] += 1
            
            # Calculate security score
            security_score = self._calculate_security_score(severity_counts, len(files))
            
            return {
                'success': True,
                'total_files': len(files),
                'files_analyzed': len([r for r in all_file_results if r.get('success')]),
                'total_vulnerabilities': len(all_vulnerabilities),
                'severity_counts': severity_counts,
                'security_score': security_score,
                'vulnerabilities': all_vulnerabilities,
                'file_results': all_file_results,
                'cross_language_vulnerabilities': len(cross_lang_vulns),
                'analysis_type': 'advanced',
                'algorithms_used': [
                    'AST Parsing',
                    'Unified IR Generation',
                    'Taint Analysis',
                    'Pattern Recognition',
                    'Cross-Language Security Detection'
                ]
            }
            
        except Exception as e:
            logger.error(f"Error in repository analysis: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _calculate_security_score(self, severity_counts: Dict[str, int], total_files: int) -> int:
        """Calculate security score (0-100) based on vulnerabilities found"""
        weighted_score = (
            severity_counts['critical'] * 25 +
            severity_counts['high'] * 15 +
            severity_counts['medium'] * 8 +
            severity_counts['low'] * 3 +
            severity_counts['info'] * 1
        )
        
        # Normalize by number of files
        if total_files > 0:
            weighted_score = weighted_score / total_files
        
        security_score = max(0, 100 - weighted_score)
        return int(security_score)
    
    def get_analysis_summary(self, result: Dict[str, Any]) -> str:
        """Generate human-readable summary of analysis"""
        if not result.get('success'):
            return f"Analysis failed: {result.get('error', 'Unknown error')}"
        
        summary_parts = [
            f"Advanced Vulnerability Analysis Complete",
            f"Files Analyzed: {result['files_analyzed']}/{result['total_files']}",
            f"Security Score: {result['security_score']}/100",
            f"Total Vulnerabilities: {result['total_vulnerabilities']}",
            "",
            "Severity Breakdown:",
            f"  Critical: {result['severity_counts']['critical']}",
            f"  High: {result['severity_counts']['high']}",
            f"  Medium: {result['severity_counts']['medium']}",
            f"  Low: {result['severity_counts']['low']}",
            f"  Info: {result['severity_counts']['info']}",
            "",
            f"Cross-Language Vulnerabilities: {result.get('cross_language_vulnerabilities', 0)}",
            "",
            "Algorithms Applied:",
        ]
        
        for algo in result.get('algorithms_used', []):
            summary_parts.append(f"  âœ“ {algo}")
        
        return "\n".join(summary_parts)
